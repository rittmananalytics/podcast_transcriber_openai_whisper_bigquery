{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "transcribe_podcast_feed.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Rittman Analytics ltd\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "XwDzTLLAY1M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Podcast Transcriber, Classifier and Speaker Labelling to BigQuery\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook contains a Python script designed to automate the process of downloading, transcribing, analyzing, and storing podcast episodes. It's particularly focused on data analytics and business intelligence podcasts, but can be adapted for other genres.\n",
        "\n",
        "##Key Features\n",
        "\n",
        "- RSS Feed Processing: Fetches podcast episode metadata from an RSS feed.\n",
        "- Audio Download and Processing: Downloads MP3 files and splits them into manageable chunks.\n",
        "- Transcription: Uses OpenAI's Whisper model to transcribe audio to text.\n",
        "- Content Analysis: Employs GPT-4 to classify episodes, generate summaries, and extract key insights.\n",
        "- Speaker Labeling: Identifies and labels speakers in the transcript.\n",
        "- Data Storage: Stores processed information in Google BigQuery for easy querying and analysis."
      ],
      "metadata": {
        "id": "53YYpklJTcO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customization\n",
        "\n",
        "The notebook can be customized by adjusting parameters such as:\n",
        "\n",
        "- The chunk size for audio splitting\n",
        "- The classification categories and tags\n",
        "- The prompts used for GPT-4 analysis\n",
        "-  The BigQuery schema to store additional or different information\n"
      ],
      "metadata": {
        "id": "JSNkFecgYaRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To use this notebook:\n",
        "\n",
        "1. Set up necessary API keys for OpenAI and Google Cloud.\n",
        "2. Configure the BigQuery dataset and table names.\n",
        "3. Specify the podcast RSS feed URL and the number of episodes to process.\n",
        "4. Run the script to process the specified number of episodes from the most recent to the oldest."
      ],
      "metadata": {
        "id": "Vi8404DiXJen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "## PIP Install Packages and dependencies"
      ],
      "metadata": {
        "id": "c7LMQawbQfRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install functions-framework==3.* requests==2.* pandas==1.* feedparser==6.* pydub==0.* openai==1.*"
      ],
      "metadata": {
        "id": "eFL3KHAGCLBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import os\n",
        "import csv\n",
        "from pydub import AudioSegment\n",
        "import openai\n",
        "import time\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.api_core import exceptions as google_exceptions"
      ],
      "metadata": {
        "id": "RcY7BBCrQud4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set your OpenAI API key\n"
      ],
      "metadata": {
        "id": "KRlLmqz6QwoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ''"
      ],
      "metadata": {
        "id": "4g3Sk68fQ57x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Google Cloud credentials"
      ],
      "metadata": {
        "id": "_PTB618TQ831"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/path/to/credentials/file'\n",
        ")"
      ],
      "metadata": {
        "id": "qaQiAZ7FRA1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize BigQuery client"
      ],
      "metadata": {
        "id": "O_kbIM1kRB38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)"
      ],
      "metadata": {
        "id": "1uRjQhzeRIDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set your BigQuery dataset and table name"
      ],
      "metadata": {
        "id": "pLzTbr3PRKAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'your-dataset-name'\n",
        "table_name = 'podcast_transcriptions'"
      ],
      "metadata": {
        "id": "eiFL39-KRO6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How It Works\n",
        "\n",
        "## Step 1 : Podcast Feed Processing\n",
        "\n",
        "The script starts by parsing the provided RSS feed URL to extract episode metadata (title, description, publication date, audio URL).\n",
        "Audio Processing:\n",
        "\n",
        "Each episode's audio file is downloaded and split into smaller chunks (default 1-minute segments) to accommodate OpenAI Whisper API limitations."
      ],
      "metadata": {
        "id": "yS75RUawTwjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_mp3(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "def split_audio(filename, chunk_length_ms=60000):  # 1 minute chunks\n",
        "    audio = AudioSegment.from_mp3(filename)\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_ms):\n",
        "        chunk = audio[i:i+chunk_length_ms]\n",
        "        chunk_file = f\"{filename}_chunk_{i//chunk_length_ms}.mp3\"\n",
        "        chunk.export(chunk_file, format=\"mp3\")\n",
        "        chunks.append(chunk_file)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Xc5B5SWWUDAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : Transcription\n",
        "\n",
        "Each audio chunk is transcribed using OpenAI's Whisper model. The transcriptions are concatenated to form the full episode transcript."
      ],
      "metadata": {
        "id": "vltUDR-rUISv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(client, filename):\n",
        "    with open(filename, \"rb\") as audio_file:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcript.text"
      ],
      "metadata": {
        "id": "pKPXQUqgUiv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 : Episode Classification\n",
        "\n",
        "The episode title is analyzed using GPT-4 to assign relevant categories and tags."
      ],
      "metadata": {
        "id": "JgHiFNNuUicb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_episode(client, title):\n",
        "    prompt = f\"\"\"\n",
        "    Classify the following podcast episode title into the most relevant category and provide additional tags.\n",
        "    Use only the categories and tags provided below. Choose one primary tag and multiple secondary tags if applicable.\n",
        "\n",
        "    Title: {title}\n",
        "\n",
        "    Categories and Tags:\n",
        "    * Data Analytics 1.1. Business Intelligence 1.1.1. Dashboards 1.1.1.1. Interactive Dashboards 1.1.1.2. KPI Dashboards 1.1.2. Reporting Tools 1.1.2.1. Looker 1.1.2.2. Lightdash 1.2. Advanced Analytics 1.2.1. Customer Analytics 1.2.1.1. Customer Segmentation 1.2.1.2. Customer Journey Analysis 1.2.2. Marketing Analytics 1.2.2.1. Attribution Modeling 1.2.2.2. Ad Spend Analysis 1.2.3. Financial Analytics 1.2.3.1. Benchmarking 1.2.3.2. Forecasting 1.3. Web and Digital Analytics 1.3.1. Event-Based Analytics 1.3.2. SEO Analytics 1.4. Specialized Analytics 1.4.1. IoT Analytics 1.4.1.1. Smart Home 1.4.1.2. Smart Buildings 1.4.2. Media Analytics\n",
        "    * Data Strategy 2.1. Modern Data Stack 2.1.1. Components 2.1.1.1. Data Integration Tools 2.1.1.2. Data Transformation Tools 2.1.2. Best Practices 2.1.2.1. Project Management 2.1.2.2. Healthchecks 2.2. Data Governance 2.2.1. Data Quality 2.2.2. Data Lineage 2.3. Cloud Strategy 2.3.1. Google Cloud 2.3.2. Oracle Cloud 2.3.3. Multi-Cloud Solutions\n",
        "    * Data Centralization 3.1. Data Warehousing 3.1.1. Cloud Data Warehouses 3.1.1.1. BigQuery 3.1.1.2. Autonomous Data Warehouse 3.1.2. Data Lakehouses 3.2. Data Modeling 3.2.1. Semantic Layers 3.2.2. Dimensional Modeling 3.2.2.1. Slowly Changing Dimensions 3.3. Data Engineering 3.3.1. ETL and Data Pipelines 3.3.2. Data Transformation 3.3.2.1. dbt 3.4. Data Integration 3.4.1. Customer Data Platforms 3.4.2. Data Synchronization\n",
        "    * Artificial Intelligence 4.1. Machine Learning 4.1.1. Predictive Analytics 4.1.2. Customer Lifetime Value 4.2. Natural Language Processing 4.2.1. Text Generation 4.2.2. Sentiment Analysis 4.3. Generative AI 4.3.1. Large Language Models 4.3.2. AI-Powered Chatbots 4.4. AI in Business Intelligence 4.4.1. Automated Insights 4.4.2. AI-Enhanced Dashboards\n",
        "\n",
        "    Output format:\n",
        "    Primary Tag: [Single most relevant tag]\n",
        "    Secondary Tags: [Comma-separated list of additional relevant tags]\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a podcast classification assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Y6Mm-AZ0UyUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 : Summary and Insights:\n",
        "\n",
        "GPT-4 generates a summary of the episode and extracts key insights from the transcript."
      ],
      "metadata": {
        "id": "p4cSAvLTU0oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_and_insights(client, transcript, guest_name):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following podcast transcript and provide a summary of the key insights, opinions, and analytics industry trends discussed. Also, identify and quote 2-3 insightful or interesting statements made by the guest speaker, {guest_name}.\n",
        "\n",
        "    Format your response as follows:\n",
        "    Summary: [A concise summary of the main points discussed in the podcast, focusing on insights, opinions, and industry trends]\n",
        "\n",
        "    Key Insights:\n",
        "    1. [First key insight or trend]\n",
        "    2. [Second key insight or trend]\n",
        "    3. [Third key insight or trend]\n",
        "\n",
        "    Notable Quotes:\n",
        "    1. \"{{First quote}}\" - {guest_name}\n",
        "    2. \"{{Second quote}}\" - {guest_name}\n",
        "    3. \"{{Third quote}}\" - {guest_name} (if available)\n",
        "\n",
        "    Transcript:\n",
        "    {transcript}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in analyzing data analytics and business intelligence podcasts.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "seVP6KLOU-fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 : Speaker Labelling\n",
        "\n",
        "The transcript is processed via GPT-4 to label dialogue with speaker names (host and guest)."
      ],
      "metadata": {
        "id": "sgJ_5xz0VBDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_speakers(client, transcript, guest_name):\n",
        "    def split_transcript(transcript, max_chunk_size=3000):\n",
        "        words = transcript.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "        for word in words:\n",
        "            if current_size + len(word) + 1 > max_chunk_size:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_size = len(word)\n",
        "            else:\n",
        "                current_chunk.append(word)\n",
        "                current_size += len(word) + 1\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "    chunks = split_transcript(transcript)\n",
        "    labeled_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt = f\"\"\"\n",
        "        Label the following chunk of podcast transcript with speaker names. The host is Mark Rittman, and the guest is {guest_name}.\n",
        "        Format the output as:\n",
        "\n",
        "        Mark Rittman: [Speaker's words]\n",
        "        {guest_name}: [Speaker's words]\n",
        "\n",
        "        Use your understanding of conversation flow and context to accurately label each part of the dialogue.\n",
        "        If you're unsure about a speaker, use your best judgment based on the content and style of speech.\n",
        "\n",
        "        This is chunk {i+1} of {len(chunks)}. Maintain consistency with previous chunks if applicable.\n",
        "\n",
        "        Transcript chunk:\n",
        "        {chunk}\n",
        "        \"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a transcript labeling assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        labeled_chunks.append(response.choices[0].message.content)\n",
        "\n",
        "    return \"\\n\".join(labeled_chunks)"
      ],
      "metadata": {
        "id": "VRlrMlQnVeNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 : Store in BigQuery\n",
        "\n",
        "All processed information (metadata, transcript, classification, summary, insights) is stored in a Google BigQuery table. The script ensures the BigQuery table exists, creating it if necessary."
      ],
      "metadata": {
        "id": "EQbP1QrSVfjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for your BigQuery table\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"title\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"link\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"description\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"published\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"audio_url\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"transcript\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"classification\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"summary_and_insights\", \"STRING\"),  # New field\n",
        "]\n",
        "\n",
        "def ensure_table_exists():\n",
        "    dataset_ref = bq_client.dataset(dataset_name)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "    try:\n",
        "        bq_client.get_table(table_ref)\n",
        "    except Exception:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        table = bq_client.create_table(table)\n",
        "        print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "\n",
        "\n",
        "\n",
        "def write_to_bigquery(episode):\n",
        "    # Ensure the table exists\n",
        "    ensure_table_exists()\n",
        "\n",
        "    # Prepare the data for BigQuery\n",
        "    row = {\n",
        "        'title': episode['title'],\n",
        "        'link': episode.get('link', ''),\n",
        "        'description': episode.get('description', '')[:1024],  # Truncate description if it's too long\n",
        "        'published': episode.get('published', ''),\n",
        "        'audio_url': episode.get('audio_url', ''),\n",
        "        'transcript': episode.get('transcript', '')[:1048576],  # Truncate transcript if it's too long (1MB limit)\n",
        "        'classification': episode.get('classification', ''),\n",
        "        'summary_and_insights': episode.get('summary_and_insights', '')[:1048576]  # New field, also truncated if necessary\n",
        "    }\n",
        "\n",
        "    # Get the table reference\n",
        "    table_ref = bq_client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "    # Load the data into BigQuery\n",
        "    try:\n",
        "        errors = bq_client.insert_rows_json(table_ref, [row])\n",
        "        if errors == []:\n",
        "            print(f\"Episode '{episode['title']}' successfully written to BigQuery\")\n",
        "        else:\n",
        "            print(f\"Errors occurred while writing episode '{episode['title']}' to BigQuery: {errors}\")\n",
        "            print(f\"Problematic row: {row}\")\n",
        "    except google_exceptions.BadRequest as e:\n",
        "        print(f\"BadRequest error for episode '{episode['title']}': {e}\")\n",
        "        print(f\"Problematic row: {row}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error for episode '{episode['title']}': {e}\")\n",
        "        print(f\"Problematic row: {row}\")\n",
        "\n",
        "# Add this function to check if rows were actually inserted\n",
        "def check_table_rows():\n",
        "    query = f\"\"\"\n",
        "    SELECT COUNT(*) as row_count\n",
        "    FROM `{bq_client.project}.{dataset_name}.{table_name}`\n",
        "    \"\"\"\n",
        "    query_job = bq_client.query(query)\n",
        "    results = query_job.result()\n",
        "    for row in results:\n",
        "        print(f\"Number of rows in the table: {row.row_count}\")"
      ],
      "metadata": {
        "id": "XzW7XB4FWde5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution and Monitoring:\n",
        "\n",
        "The main function orchestrates the processing of multiple episodes. It provides progress updates and a final count of processed episodes. A check is performed to verify the number of rows inserted into BigQuery."
      ],
      "metadata": {
        "id": "kW0Ov4hZWpWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_podcast(url):\n",
        "    feed = feedparser.parse(url)\n",
        "    episodes = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        episode = {\n",
        "            'title': entry.get('title', ''),\n",
        "            'link': entry.get('link', ''),\n",
        "            'description': entry.get('description', ''),\n",
        "            'published': entry.get('published', ''),\n",
        "        }\n",
        "\n",
        "        audio_url = None\n",
        "        if 'links' in entry:\n",
        "            for link in entry.links:\n",
        "                if 'type' in link and link['type'].startswith('audio/'):\n",
        "                    audio_url = link.get('href')\n",
        "                    break\n",
        "\n",
        "        if not audio_url and 'enclosures' in entry:\n",
        "            for enclosure in entry.enclosures:\n",
        "                if 'type' in enclosure and enclosure['type'].startswith('audio/'):\n",
        "                    audio_url = enclosure.get('href')\n",
        "                    break\n",
        "\n",
        "        if audio_url:\n",
        "            episode['audio_url'] = audio_url\n",
        "            episodes.append(episode)\n",
        "\n",
        "    return episodes\n",
        "\n",
        "\n",
        "\n",
        "def process_episode(client, episode):\n",
        "    print(f\"Processing episode: {episode['title']}\")\n",
        "\n",
        "    filename = f\"temp_{episode['title'].replace(' ', '_')}.mp3\"\n",
        "    download_mp3(episode['audio_url'], filename)\n",
        "\n",
        "    chunks = split_audio(filename)\n",
        "\n",
        "    full_transcript = \"\"\n",
        "    for chunk in chunks:\n",
        "        transcript = transcribe_audio(client, chunk)\n",
        "        full_transcript += transcript + \" \"\n",
        "        os.remove(chunk)\n",
        "\n",
        "    classification = classify_episode(client, episode['title'])\n",
        "\n",
        "    guest_name = episode['title'].split('with')[-1].strip() if 'with' in episode['title'] else \"Guest\"\n",
        "    labeled_transcript = label_speakers(client, full_transcript, guest_name)\n",
        "\n",
        "    summary_and_insights = generate_summary_and_insights(client, labeled_transcript, guest_name)\n",
        "\n",
        "    episode['transcript'] = labeled_transcript\n",
        "    episode['classification'] = classification\n",
        "    episode['summary_and_insights'] = summary_and_insights\n",
        "\n",
        "    os.remove(filename)\n",
        "\n",
        "    # Write the processed episode to BigQuery\n",
        "    write_to_bigquery(episode)\n",
        "\n",
        "    return episode\n",
        "\n",
        "def main(feed_url, num_episodes):\n",
        "    client = openai.OpenAI(api_key='')\n",
        "\n",
        "    # Ensure the BigQuery table exists\n",
        "    ensure_table_exists()\n",
        "\n",
        "    episodes = process_podcast(feed_url)[:num_episodes]\n",
        "\n",
        "    processed_episodes = []\n",
        "    for episode in episodes:\n",
        "        processed_episode = process_episode(client, episode)\n",
        "        processed_episodes.append(processed_episode)\n",
        "\n",
        "    check_table_rows()\n",
        "\n",
        "    return f\"Processed {len(processed_episodes)} episodes and wrote them to BigQuery\"\n",
        "\n",
        "# Example usage\n"
      ],
      "metadata": {
        "id": "6z55M-MEWsBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_url = \"https://mark-rittman-lzl4.squarespace.com/podcast?format=rss\"\n",
        "num_episodes = 125\n",
        "\n",
        "result = main(feed_url, num_episodes)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZWu0NZKnXHu8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}