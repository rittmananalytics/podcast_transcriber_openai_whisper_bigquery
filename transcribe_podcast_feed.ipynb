{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "transcribe_podcast_feed.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install functions-framework==3.* requests==2.* pandas==1.* feedparser==6.* pydub==0.* openai==1.*"
      ],
      "metadata": {
        "id": "eFL3KHAGCLBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import os\n",
        "import csv\n",
        "from pydub import AudioSegment\n",
        "import openai\n",
        "import time\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.api_core import exceptions as google_exceptions\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = ''\n",
        "\n",
        "# Set up Google Cloud credentials\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/path/to/credentials/file'\n",
        ")\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
        "\n",
        "# Set your BigQuery dataset and table name\n",
        "dataset_name = 'your-dataset-name'\n",
        "table_name = 'podcast_transcriptions'\n",
        "\n",
        "def download_mp3(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "def split_audio(filename, chunk_length_ms=60000):  # 1 minute chunks\n",
        "    audio = AudioSegment.from_mp3(filename)\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_ms):\n",
        "        chunk = audio[i:i+chunk_length_ms]\n",
        "        chunk_file = f\"{filename}_chunk_{i//chunk_length_ms}.mp3\"\n",
        "        chunk.export(chunk_file, format=\"mp3\")\n",
        "        chunks.append(chunk_file)\n",
        "    return chunks\n",
        "\n",
        "def transcribe_audio(client, filename):\n",
        "    with open(filename, \"rb\") as audio_file:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcript.text\n",
        "\n",
        "def classify_episode(client, title):\n",
        "    prompt = f\"\"\"\n",
        "    Classify the following podcast episode title into the most relevant category and provide additional tags.\n",
        "    Use only the categories and tags provided below. Choose one primary tag and multiple secondary tags if applicable.\n",
        "\n",
        "    Title: {title}\n",
        "\n",
        "    Categories and Tags:\n",
        "    * Data Analytics 1.1. Business Intelligence 1.1.1. Dashboards 1.1.1.1. Interactive Dashboards 1.1.1.2. KPI Dashboards 1.1.2. Reporting Tools 1.1.2.1. Looker 1.1.2.2. Lightdash 1.2. Advanced Analytics 1.2.1. Customer Analytics 1.2.1.1. Customer Segmentation 1.2.1.2. Customer Journey Analysis 1.2.2. Marketing Analytics 1.2.2.1. Attribution Modeling 1.2.2.2. Ad Spend Analysis 1.2.3. Financial Analytics 1.2.3.1. Benchmarking 1.2.3.2. Forecasting 1.3. Web and Digital Analytics 1.3.1. Event-Based Analytics 1.3.2. SEO Analytics 1.4. Specialized Analytics 1.4.1. IoT Analytics 1.4.1.1. Smart Home 1.4.1.2. Smart Buildings 1.4.2. Media Analytics\n",
        "    * Data Strategy 2.1. Modern Data Stack 2.1.1. Components 2.1.1.1. Data Integration Tools 2.1.1.2. Data Transformation Tools 2.1.2. Best Practices 2.1.2.1. Project Management 2.1.2.2. Healthchecks 2.2. Data Governance 2.2.1. Data Quality 2.2.2. Data Lineage 2.3. Cloud Strategy 2.3.1. Google Cloud 2.3.2. Oracle Cloud 2.3.3. Multi-Cloud Solutions\n",
        "    * Data Centralization 3.1. Data Warehousing 3.1.1. Cloud Data Warehouses 3.1.1.1. BigQuery 3.1.1.2. Autonomous Data Warehouse 3.1.2. Data Lakehouses 3.2. Data Modeling 3.2.1. Semantic Layers 3.2.2. Dimensional Modeling 3.2.2.1. Slowly Changing Dimensions 3.3. Data Engineering 3.3.1. ETL and Data Pipelines 3.3.2. Data Transformation 3.3.2.1. dbt 3.4. Data Integration 3.4.1. Customer Data Platforms 3.4.2. Data Synchronization\n",
        "    * Artificial Intelligence 4.1. Machine Learning 4.1.1. Predictive Analytics 4.1.2. Customer Lifetime Value 4.2. Natural Language Processing 4.2.1. Text Generation 4.2.2. Sentiment Analysis 4.3. Generative AI 4.3.1. Large Language Models 4.3.2. AI-Powered Chatbots 4.4. AI in Business Intelligence 4.4.1. Automated Insights 4.4.2. AI-Enhanced Dashboards\n",
        "\n",
        "    Output format:\n",
        "    Primary Tag: [Single most relevant tag]\n",
        "    Secondary Tags: [Comma-separated list of additional relevant tags]\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a podcast classification assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def generate_summary_and_insights(client, transcript, guest_name):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following podcast transcript and provide a summary of the key insights, opinions, and analytics industry trends discussed. Also, identify and quote 2-3 insightful or interesting statements made by the guest speaker, {guest_name}.\n",
        "\n",
        "    Format your response as follows:\n",
        "    Summary: [A concise summary of the main points discussed in the podcast, focusing on insights, opinions, and industry trends]\n",
        "\n",
        "    Key Insights:\n",
        "    1. [First key insight or trend]\n",
        "    2. [Second key insight or trend]\n",
        "    3. [Third key insight or trend]\n",
        "\n",
        "    Notable Quotes:\n",
        "    1. \"{{First quote}}\" - {guest_name}\n",
        "    2. \"{{Second quote}}\" - {guest_name}\n",
        "    3. \"{{Third quote}}\" - {guest_name} (if available)\n",
        "\n",
        "    Transcript:\n",
        "    {transcript}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in analyzing data analytics and business intelligence podcasts.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def label_speakers(client, transcript, guest_name):\n",
        "    def split_transcript(transcript, max_chunk_size=3000):\n",
        "        words = transcript.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "        for word in words:\n",
        "            if current_size + len(word) + 1 > max_chunk_size:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_size = len(word)\n",
        "            else:\n",
        "                current_chunk.append(word)\n",
        "                current_size += len(word) + 1\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "    chunks = split_transcript(transcript)\n",
        "    labeled_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt = f\"\"\"\n",
        "        Label the following chunk of podcast transcript with speaker names. The host is Mark Rittman, and the guest is {guest_name}.\n",
        "        Format the output as:\n",
        "\n",
        "        Mark Rittman: [Speaker's words]\n",
        "        {guest_name}: [Speaker's words]\n",
        "\n",
        "        Use your understanding of conversation flow and context to accurately label each part of the dialogue.\n",
        "        If you're unsure about a speaker, use your best judgment based on the content and style of speech.\n",
        "\n",
        "        This is chunk {i+1} of {len(chunks)}. Maintain consistency with previous chunks if applicable.\n",
        "\n",
        "        Transcript chunk:\n",
        "        {chunk}\n",
        "        \"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a transcript labeling assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        labeled_chunks.append(response.choices[0].message.content)\n",
        "\n",
        "    return \"\\n\".join(labeled_chunks)\n",
        "\n",
        "def process_podcast(url):\n",
        "    feed = feedparser.parse(url)\n",
        "    episodes = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        episode = {\n",
        "            'title': entry.get('title', ''),\n",
        "            'link': entry.get('link', ''),\n",
        "            'description': entry.get('description', ''),\n",
        "            'published': entry.get('published', ''),\n",
        "        }\n",
        "\n",
        "        audio_url = None\n",
        "        if 'links' in entry:\n",
        "            for link in entry.links:\n",
        "                if 'type' in link and link['type'].startswith('audio/'):\n",
        "                    audio_url = link.get('href')\n",
        "                    break\n",
        "\n",
        "        if not audio_url and 'enclosures' in entry:\n",
        "            for enclosure in entry.enclosures:\n",
        "                if 'type' in enclosure and enclosure['type'].startswith('audio/'):\n",
        "                    audio_url = enclosure.get('href')\n",
        "                    break\n",
        "\n",
        "        if audio_url:\n",
        "            episode['audio_url'] = audio_url\n",
        "            episodes.append(episode)\n",
        "\n",
        "    return episodes\n",
        "\n",
        "# Define the schema for your BigQuery table\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"title\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"link\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"description\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"published\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"audio_url\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"transcript\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"classification\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"summary_and_insights\", \"STRING\"),  # New field\n",
        "]\n",
        "\n",
        "def ensure_table_exists():\n",
        "    dataset_ref = bq_client.dataset(dataset_name)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "    try:\n",
        "        bq_client.get_table(table_ref)\n",
        "    except Exception:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        table = bq_client.create_table(table)\n",
        "        print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "\n",
        "\n",
        "\n",
        "def write_to_bigquery(episode):\n",
        "    # Ensure the table exists\n",
        "    ensure_table_exists()\n",
        "\n",
        "    # Prepare the data for BigQuery\n",
        "    row = {\n",
        "        'title': episode['title'],\n",
        "        'link': episode.get('link', ''),\n",
        "        'description': episode.get('description', '')[:1024],  # Truncate description if it's too long\n",
        "        'published': episode.get('published', ''),\n",
        "        'audio_url': episode.get('audio_url', ''),\n",
        "        'transcript': episode.get('transcript', '')[:1048576],  # Truncate transcript if it's too long (1MB limit)\n",
        "        'classification': episode.get('classification', ''),\n",
        "        'summary_and_insights': episode.get('summary_and_insights', '')[:1048576]  # New field, also truncated if necessary\n",
        "    }\n",
        "\n",
        "    # Get the table reference\n",
        "    table_ref = bq_client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "    # Load the data into BigQuery\n",
        "    try:\n",
        "        errors = bq_client.insert_rows_json(table_ref, [row])\n",
        "        if errors == []:\n",
        "            print(f\"Episode '{episode['title']}' successfully written to BigQuery\")\n",
        "        else:\n",
        "            print(f\"Errors occurred while writing episode '{episode['title']}' to BigQuery: {errors}\")\n",
        "            print(f\"Problematic row: {row}\")\n",
        "    except google_exceptions.BadRequest as e:\n",
        "        print(f\"BadRequest error for episode '{episode['title']}': {e}\")\n",
        "        print(f\"Problematic row: {row}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error for episode '{episode['title']}': {e}\")\n",
        "        print(f\"Problematic row: {row}\")\n",
        "\n",
        "# Add this function to check if rows were actually inserted\n",
        "def check_table_rows():\n",
        "    query = f\"\"\"\n",
        "    SELECT COUNT(*) as row_count\n",
        "    FROM `{bq_client.project}.{dataset_name}.{table_name}`\n",
        "    \"\"\"\n",
        "    query_job = bq_client.query(query)\n",
        "    results = query_job.result()\n",
        "    for row in results:\n",
        "        print(f\"Number of rows in the table: {row.row_count}\")\n",
        "\n",
        "def process_episode(client, episode):\n",
        "    print(f\"Processing episode: {episode['title']}\")\n",
        "\n",
        "    filename = f\"temp_{episode['title'].replace(' ', '_')}.mp3\"\n",
        "    download_mp3(episode['audio_url'], filename)\n",
        "\n",
        "    chunks = split_audio(filename)\n",
        "\n",
        "    full_transcript = \"\"\n",
        "    for chunk in chunks:\n",
        "        transcript = transcribe_audio(client, chunk)\n",
        "        full_transcript += transcript + \" \"\n",
        "        os.remove(chunk)\n",
        "\n",
        "    classification = classify_episode(client, episode['title'])\n",
        "\n",
        "    guest_name = episode['title'].split('with')[-1].strip() if 'with' in episode['title'] else \"Guest\"\n",
        "    labeled_transcript = label_speakers(client, full_transcript, guest_name)\n",
        "\n",
        "    summary_and_insights = generate_summary_and_insights(client, labeled_transcript, guest_name)\n",
        "\n",
        "    episode['transcript'] = labeled_transcript\n",
        "    episode['classification'] = classification\n",
        "    episode['summary_and_insights'] = summary_and_insights\n",
        "\n",
        "    os.remove(filename)\n",
        "\n",
        "    # Write the processed episode to BigQuery\n",
        "    write_to_bigquery(episode)\n",
        "\n",
        "    return episode\n",
        "\n",
        "def main(feed_url, num_episodes):\n",
        "    client = openai.OpenAI(api_key='')\n",
        "\n",
        "    # Ensure the BigQuery table exists\n",
        "    ensure_table_exists()\n",
        "\n",
        "    episodes = process_podcast(feed_url)[:num_episodes]\n",
        "\n",
        "    processed_episodes = []\n",
        "    for episode in episodes:\n",
        "        processed_episode = process_episode(client, episode)\n",
        "        processed_episodes.append(processed_episode)\n",
        "\n",
        "    check_table_rows()\n",
        "\n",
        "    return f\"Processed {len(processed_episodes)} episodes and wrote them to BigQuery\"\n",
        "\n",
        "# Example usage\n",
        "feed_url = \"https://mark-rittman-lzl4.squarespace.com/podcast?format=rss\"\n",
        "num_episodes = 125\n",
        "\n",
        "result = main(feed_url, num_episodes)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "6z55M-MEWsBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
