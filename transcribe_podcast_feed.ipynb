{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "transcribe_podcast_feed"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install functions-framework==3.* requests==2.* pandas==1.* feedparser==6.* pydub==0.* openai==1.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eFL3KHAGCLBC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721232500036,
          "user_tz": -60,
          "elapsed": 7418,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7f3781f8-b65e-443b-f377-4fe5ef5e9d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: functions-framework==3.* in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: requests==2.* in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: pandas==1.* in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: feedparser==6.* in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: pydub==0.* in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: openai==1.* in /usr/local/lib/python3.10/dist-packages (1.35.14)\n",
            "Requirement already satisfied: flask<4.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (2.2.5)\n",
            "Requirement already satisfied: click<9.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (8.1.7)\n",
            "Requirement already satisfied: watchdog>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (4.0.1)\n",
            "Requirement already satisfied: cloudevents<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (1.11.0)\n",
            "Requirement already satisfied: Werkzeug<4.0.0,>=0.14 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (3.0.3)\n",
            "Requirement already satisfied: gunicorn>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (22.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.*) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.*) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.*) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.*) (1.25.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.*) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.*) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.*) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.*) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.*) (1.10.17)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.*) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.*) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.*) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.*) (1.2.1)\n",
            "Requirement already satisfied: deprecation<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cloudevents<2.0.0,>=1.2.0->functions-framework==3.*) (2.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask<4.0,>=1.0->functions-framework==3.*) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask<4.0,>=1.0->functions-framework==3.*) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=19.2.0->functions-framework==3.*) (24.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.*) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.*) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.*) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<4.0.0,>=0.14->functions-framework==3.*) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import os\n",
        "import csv\n",
        "from pydub import AudioSegment\n",
        "import openai\n",
        "import time\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.api_core import exceptions as google_exceptions\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = ''\n",
        "\n",
        "# Set up Google Cloud credentials\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/path/to/credentials/file'\n",
        ")\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
        "\n",
        "# Set your BigQuery dataset and table name\n",
        "dataset_name = 'your-dataset-name'\n",
        "table_name = 'podcast_transcriptions'\n",
        "\n",
        "def download_mp3(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "def split_audio(filename, chunk_length_ms=60000):  # 1 minute chunks\n",
        "    audio = AudioSegment.from_mp3(filename)\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_ms):\n",
        "        chunk = audio[i:i+chunk_length_ms]\n",
        "        chunk_file = f\"{filename}_chunk_{i//chunk_length_ms}.mp3\"\n",
        "        chunk.export(chunk_file, format=\"mp3\")\n",
        "        chunks.append(chunk_file)\n",
        "    return chunks\n",
        "\n",
        "def transcribe_audio(client, filename):\n",
        "    with open(filename, \"rb\") as audio_file:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcript.text\n",
        "\n",
        "def classify_episode(client, title):\n",
        "    prompt = f\"\"\"\n",
        "    Classify the following podcast episode title into the most relevant category and provide additional tags.\n",
        "    Use only the categories and tags provided below. Choose one primary tag and multiple secondary tags if applicable.\n",
        "\n",
        "    Title: {title}\n",
        "\n",
        "    Categories and Tags:\n",
        "    * Data Analytics 1.1. Business Intelligence 1.1.1. Dashboards 1.1.1.1. Interactive Dashboards 1.1.1.2. KPI Dashboards 1.1.2. Reporting Tools 1.1.2.1. Looker 1.1.2.2. Lightdash 1.2. Advanced Analytics 1.2.1. Customer Analytics 1.2.1.1. Customer Segmentation 1.2.1.2. Customer Journey Analysis 1.2.2. Marketing Analytics 1.2.2.1. Attribution Modeling 1.2.2.2. Ad Spend Analysis 1.2.3. Financial Analytics 1.2.3.1. Benchmarking 1.2.3.2. Forecasting 1.3. Web and Digital Analytics 1.3.1. Event-Based Analytics 1.3.2. SEO Analytics 1.4. Specialized Analytics 1.4.1. IoT Analytics 1.4.1.1. Smart Home 1.4.1.2. Smart Buildings 1.4.2. Media Analytics\n",
        "    * Data Strategy 2.1. Modern Data Stack 2.1.1. Components 2.1.1.1. Data Integration Tools 2.1.1.2. Data Transformation Tools 2.1.2. Best Practices 2.1.2.1. Project Management 2.1.2.2. Healthchecks 2.2. Data Governance 2.2.1. Data Quality 2.2.2. Data Lineage 2.3. Cloud Strategy 2.3.1. Google Cloud 2.3.2. Oracle Cloud 2.3.3. Multi-Cloud Solutions\n",
        "    * Data Centralization 3.1. Data Warehousing 3.1.1. Cloud Data Warehouses 3.1.1.1. BigQuery 3.1.1.2. Autonomous Data Warehouse 3.1.2. Data Lakehouses 3.2. Data Modeling 3.2.1. Semantic Layers 3.2.2. Dimensional Modeling 3.2.2.1. Slowly Changing Dimensions 3.3. Data Engineering 3.3.1. ETL and Data Pipelines 3.3.2. Data Transformation 3.3.2.1. dbt 3.4. Data Integration 3.4.1. Customer Data Platforms 3.4.2. Data Synchronization\n",
        "    * Artificial Intelligence 4.1. Machine Learning 4.1.1. Predictive Analytics 4.1.2. Customer Lifetime Value 4.2. Natural Language Processing 4.2.1. Text Generation 4.2.2. Sentiment Analysis 4.3. Generative AI 4.3.1. Large Language Models 4.3.2. AI-Powered Chatbots 4.4. AI in Business Intelligence 4.4.1. Automated Insights 4.4.2. AI-Enhanced Dashboards\n",
        "\n",
        "    Output format:\n",
        "    Primary Tag: [Single most relevant tag]\n",
        "    Secondary Tags: [Comma-separated list of additional relevant tags]\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a podcast classification assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def generate_summary_and_insights(client, transcript, guest_name):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following podcast transcript and provide a summary of the key insights, opinions, and analytics industry trends discussed. Also, identify and quote 2-3 insightful or interesting statements made by the guest speaker, {guest_name}.\n",
        "\n",
        "    Format your response as follows:\n",
        "    Summary: [A concise summary of the main points discussed in the podcast, focusing on insights, opinions, and industry trends]\n",
        "\n",
        "    Key Insights:\n",
        "    1. [First key insight or trend]\n",
        "    2. [Second key insight or trend]\n",
        "    3. [Third key insight or trend]\n",
        "\n",
        "    Notable Quotes:\n",
        "    1. \"{{First quote}}\" - {guest_name}\n",
        "    2. \"{{Second quote}}\" - {guest_name}\n",
        "    3. \"{{Third quote}}\" - {guest_name} (if available)\n",
        "\n",
        "    Transcript:\n",
        "    {transcript}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in analyzing data analytics and business intelligence podcasts.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def label_speakers(client, transcript, guest_name):\n",
        "    def split_transcript(transcript, max_chunk_size=3000):\n",
        "        words = transcript.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "        for word in words:\n",
        "            if current_size + len(word) + 1 > max_chunk_size:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_size = len(word)\n",
        "            else:\n",
        "                current_chunk.append(word)\n",
        "                current_size += len(word) + 1\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "    chunks = split_transcript(transcript)\n",
        "    labeled_chunks = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        prompt = f\"\"\"\n",
        "        Label the following chunk of podcast transcript with speaker names. The host is Mark Rittman, and the guest is {guest_name}.\n",
        "        Format the output as:\n",
        "\n",
        "        Mark Rittman: [Speaker's words]\n",
        "        {guest_name}: [Speaker's words]\n",
        "\n",
        "        Use your understanding of conversation flow and context to accurately label each part of the dialogue.\n",
        "        If you're unsure about a speaker, use your best judgment based on the content and style of speech.\n",
        "\n",
        "        This is chunk {i+1} of {len(chunks)}. Maintain consistency with previous chunks if applicable.\n",
        "\n",
        "        Transcript chunk:\n",
        "        {chunk}\n",
        "        \"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a transcript labeling assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        labeled_chunks.append(response.choices[0].message.content)\n",
        "\n",
        "    return \"\\n\".join(labeled_chunks)\n",
        "\n",
        "def process_podcast(url):\n",
        "    feed = feedparser.parse(url)\n",
        "    episodes = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        episode = {\n",
        "            'title': entry.get('title', ''),\n",
        "            'link': entry.get('link', ''),\n",
        "            'description': entry.get('description', ''),\n",
        "            'published': entry.get('published', ''),\n",
        "        }\n",
        "\n",
        "        audio_url = None\n",
        "        if 'links' in entry:\n",
        "            for link in entry.links:\n",
        "                if 'type' in link and link['type'].startswith('audio/'):\n",
        "                    audio_url = link.get('href')\n",
        "                    break\n",
        "\n",
        "        if not audio_url and 'enclosures' in entry:\n",
        "            for enclosure in entry.enclosures:\n",
        "                if 'type' in enclosure and enclosure['type'].startswith('audio/'):\n",
        "                    audio_url = enclosure.get('href')\n",
        "                    break\n",
        "\n",
        "        if audio_url:\n",
        "            episode['audio_url'] = audio_url\n",
        "            episodes.append(episode)\n",
        "\n",
        "    return episodes\n",
        "\n",
        "# Define the schema for your BigQuery table\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"title\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"link\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"description\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"published\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"audio_url\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"transcript\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"classification\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"summary_and_insights\", \"STRING\"),  # New field\n",
        "]\n",
        "\n",
        "def ensure_table_exists():\n",
        "    dataset_ref = bq_client.dataset(dataset_name)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "    try:\n",
        "        bq_client.get_table(table_ref)\n",
        "    except Exception:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        table = bq_client.create_table(table)\n",
        "        print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "\n",
        "\n",
        "\n",
        "def write_to_bigquery(episode):\n",
        "    # Ensure the table exists\n",
        "    ensure_table_exists()\n",
        "\n",
        "    # Prepare the data for BigQuery\n",
        "    row = {\n",
        "        'title': episode['title'],\n",
        "        'link': episode.get('link', ''),\n",
        "        'description': episode.get('description', '')[:1024],  # Truncate description if it's too long\n",
        "        'published': episode.get('published', ''),\n",
        "        'audio_url': episode.get('audio_url', ''),\n",
        "        'transcript': episode.get('transcript', '')[:1048576],  # Truncate transcript if it's too long (1MB limit)\n",
        "        'classification': episode.get('classification', ''),\n",
        "        'summary_and_insights': episode.get('summary_and_insights', '')[:1048576]  # New field, also truncated if necessary\n",
        "    }\n",
        "\n",
        "    # Get the table reference\n",
        "    table_ref = bq_client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "    # Load the data into BigQuery\n",
        "    try:\n",
        "        errors = bq_client.insert_rows_json(table_ref, [row])\n",
        "        if errors == []:\n",
        "            print(f\"Episode '{episode['title']}' successfully written to BigQuery\")\n",
        "        else:\n",
        "            print(f\"Errors occurred while writing episode '{episode['title']}' to BigQuery: {errors}\")\n",
        "            print(f\"Problematic row: {row}\")\n",
        "    except google_exceptions.BadRequest as e:\n",
        "        print(f\"BadRequest error for episode '{episode['title']}': {e}\")\n",
        "        print(f\"Problematic row: {row}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error for episode '{episode['title']}': {e}\")\n",
        "        print(f\"Problematic row: {row}\")\n",
        "\n",
        "# Add this function to check if rows were actually inserted\n",
        "def check_table_rows():\n",
        "    query = f\"\"\"\n",
        "    SELECT COUNT(*) as row_count\n",
        "    FROM `{bq_client.project}.{dataset_name}.{table_name}`\n",
        "    \"\"\"\n",
        "    query_job = bq_client.query(query)\n",
        "    results = query_job.result()\n",
        "    for row in results:\n",
        "        print(f\"Number of rows in the table: {row.row_count}\")\n",
        "\n",
        "def process_episode(client, episode):\n",
        "    print(f\"Processing episode: {episode['title']}\")\n",
        "\n",
        "    filename = f\"temp_{episode['title'].replace(' ', '_')}.mp3\"\n",
        "    download_mp3(episode['audio_url'], filename)\n",
        "\n",
        "    chunks = split_audio(filename)\n",
        "\n",
        "    full_transcript = \"\"\n",
        "    for chunk in chunks:\n",
        "        transcript = transcribe_audio(client, chunk)\n",
        "        full_transcript += transcript + \" \"\n",
        "        os.remove(chunk)\n",
        "\n",
        "    classification = classify_episode(client, episode['title'])\n",
        "\n",
        "    guest_name = episode['title'].split('with')[-1].strip() if 'with' in episode['title'] else \"Guest\"\n",
        "    labeled_transcript = label_speakers(client, full_transcript, guest_name)\n",
        "\n",
        "    summary_and_insights = generate_summary_and_insights(client, labeled_transcript, guest_name)\n",
        "\n",
        "    episode['transcript'] = labeled_transcript\n",
        "    episode['classification'] = classification\n",
        "    episode['summary_and_insights'] = summary_and_insights\n",
        "\n",
        "    os.remove(filename)\n",
        "\n",
        "    # Write the processed episode to BigQuery\n",
        "    write_to_bigquery(episode)\n",
        "\n",
        "    return episode\n",
        "\n",
        "def main(feed_url, num_episodes):\n",
        "    client = openai.OpenAI(api_key='')\n",
        "\n",
        "    # Ensure the BigQuery table exists\n",
        "    ensure_table_exists()\n",
        "\n",
        "    episodes = process_podcast(feed_url)[:num_episodes]\n",
        "\n",
        "    processed_episodes = []\n",
        "    for episode in episodes:\n",
        "        processed_episode = process_episode(client, episode)\n",
        "        processed_episodes.append(processed_episode)\n",
        "\n",
        "    check_table_rows()\n",
        "\n",
        "    return f\"Processed {len(processed_episodes)} episodes and wrote them to BigQuery\"\n",
        "\n",
        "# Example usage\n",
        "feed_url = \"https://mark-rittman-lzl4.squarespace.com/podcast?format=rss\"\n",
        "num_episodes = 125\n",
        "\n",
        "result = main(feed_url, num_episodes)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6z55M-MEWsBB",
        "outputId": "21f6f054-20bf-4a7b-e14e-f76c41764a98",
        "executionInfo": {
          "status": "error",
          "timestamp": 1721252052810,
          "user_tz": -60,
          "elapsed": 18547681,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing episode: Drill to Detail Ep.80 'Data Architecture and Data Teams at Hubspot' with Special Guest James Densmore\n",
            "Episode 'Drill to Detail Ep.80 'Data Architecture and Data Teams at Hubspot' with Special Guest James Densmore' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.79 'Scaling the Modern Data Analytics Stack' with Special Guests Drew Banin and Stewart Bryson\n",
            "Episode 'Drill to Detail Ep.79 'Scaling the Modern Data Analytics Stack' with Special Guests Drew Banin and Stewart Bryson' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.78 ‘Coronavirus, Shutdowns and the Data Analytics Industry’ with Special Guest Seth Rosen\n",
            "Episode 'Drill to Detail Ep.78 ‘Coronavirus, Shutdowns and the Data Analytics Industry’ with Special Guest Seth Rosen' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.77 ' Keboola, Scaling Analytics and Winning the Looker Join Hackathon' with Special Guest Pavel Dolezal\n",
            "Episode 'Drill to Detail Ep.77 ' Keboola, Scaling Analytics and Winning the Looker Join Hackathon' with Special Guest Pavel Dolezal' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.76 'Segment, Ecosystems and Customer Data Platforms' with Special Guest Calvin French-Owen\n",
            "Episode 'Drill to Detail Ep.76 'Segment, Ecosystems and Customer Data Platforms' with Special Guest Calvin French-Owen' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.75 'Bootstrapping an Early-Stage SaaS Analytics Consultancy' with Special Guest Olivier Dupuis\n",
            "Episode 'Drill to Detail Ep.75 'Bootstrapping an Early-Stage SaaS Analytics Consultancy' with Special Guest Olivier Dupuis' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.74 'Accelerating Oracle from Express to Analytic Views' with Special Guest Bud Endress\n",
            "Episode 'Drill to Detail Ep.74 'Accelerating Oracle from Express to Analytic Views' with Special Guest Bud Endress' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep. 73 'Luck, Thinking Different and Designing Looker Data Platform' with Special Guest Colin Zima\n",
            "Episode 'Drill to Detail Ep. 73 'Luck, Thinking Different and Designing Looker Data Platform' with Special Guest Colin Zima' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.72 'Conversion Rate Optimization and Other CRAP' with Special Guest Bhav Patel\n",
            "Episode 'Drill to Detail Ep.72 'Conversion Rate Optimization and Other CRAP' with Special Guest Bhav Patel' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.71 'The Rise of Snowflake Data Warehouse' With Special Guest Kent Graziano\n",
            "Episode 'Drill to Detail Ep.71 'The Rise of Snowflake Data Warehouse' With Special Guest Kent Graziano' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.70 'Oracle Analytics, Luke Skywalker and the Remarkable Return of Enterprise Analytics' featuring Special Guest Bruno Aziza\n",
            "Episode 'Drill to Detail Ep.70 'Oracle Analytics, Luke Skywalker and the Remarkable Return of Enterprise Analytics' featuring Special Guest Bruno Aziza' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.69 'Looker, Tableau and Consolidation in the BI Industry' featuring Special Guests Tristan Handy and Stewart Bryson\n",
            "Episode 'Drill to Detail Ep.69 'Looker, Tableau and Consolidation in the BI Industry' featuring Special Guests Tristan Handy and Stewart Bryson' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.68 ‘Confluent, Event-First Thinking and Streaming Real-Time Analytics’ With Special Guests Robin Moffatt and Ricardo Ferreira and Special Host Stewart Bryson\n",
            "Episode 'Drill to Detail Ep.68 ‘Confluent, Event-First Thinking and Streaming Real-Time Analytics’ With Special Guests Robin Moffatt and Ricardo Ferreira and Special Host Stewart Bryson' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.67 'Thinking Probabilistically and Scaling-Out the Modern BI Stack' with Special Guest Dylan Baker\n",
            "Episode 'Drill to Detail Ep.67 'Thinking Probabilistically and Scaling-Out the Modern BI Stack' with Special Guest Dylan Baker' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.66 'ETL, Incorta and the Death of the Star Schema' with Special Guest Matthew Halliday\n",
            "Episode 'Drill to Detail Ep.66 'ETL, Incorta and the Death of the Star Schema' with Special Guest Matthew Halliday' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.65 'Bootstrapping, Growth Hacking and Supermetrics Data Pipelines for Digital Marketers' with Special Guest Mikael Thuneberg\n",
            "Episode 'Drill to Detail Ep.65 'Bootstrapping, Growth Hacking and Supermetrics Data Pipelines for Digital Marketers' with Special Guest Mikael Thuneberg' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.64 ‘Google BigQuery, BI Engine and the Future of Data Warehousing’ with Special Guest Jordan Tigani\n",
            "Episode 'Drill to Detail Ep.64 ‘Google BigQuery, BI Engine and the Future of Data Warehousing’ with Special Guest Jordan Tigani' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.63 'Stitch, Talend and the Evolution of Data Pipelines' with Special Guest Jake Stein\n",
            "Episode 'Drill to Detail Ep.63 'Stitch, Talend and the Evolution of Data Pipelines' with Special Guest Jake Stein' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.62 'Enterprise analytics, DW Modernization and the rediscovery of Data Governance' With Special Guest Mike Ferguson\n",
            "Episode 'Drill to Detail Ep.62 'Enterprise analytics, DW Modernization and the rediscovery of Data Governance' With Special Guest Mike Ferguson' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.61 'Kaggle, DataRobot and How to Be a ... Zillionaire!' With Special Guest Jordan Meyer\n",
            "Episode 'Drill to Detail Ep.61 'Kaggle, DataRobot and How to Be a ... Zillionaire!' With Special Guest Jordan Meyer' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.60 'A Deeper Look Into Looker' With Special Guest Lloyd Tabb\n",
            "Episode 'Drill to Detail Ep.60 'A Deeper Look Into Looker' With Special Guest Lloyd Tabb' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.59 'Looker, dbt and Digital Analytics Today' With Special Guests Tristan Handy and Stewart Bryson\n",
            "Episode 'Drill to Detail Ep.59 'Looker, dbt and Digital Analytics Today' With Special Guests Tristan Handy and Stewart Bryson' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.58 'Candy Crush Saga, Analytics and Mobile Gaming' With Special Guest Jonathan Palmer\n",
            "Episode 'Drill to Detail Ep.58 'Candy Crush Saga, Analytics and Mobile Gaming' With Special Guest Jonathan Palmer' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.57 'WhereScape Red and Data Warehouse Automation' With Special Guest Neil Barton\n",
            "Episode 'Drill to Detail Ep.57 'WhereScape Red and Data Warehouse Automation' With Special Guest Neil Barton' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.56 'Essbase Cloud, Analytics Cloud and Cloud Analytics Consulting' with Special Guest Matt Yorke\n",
            "Episode 'Drill to Detail Ep.56 'Essbase Cloud, Analytics Cloud and Cloud Analytics Consulting' with Special Guest Matt Yorke' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.55 'Snowplow, Data Pipelines and Event-Level Digital Analytics' with Special Guest Yali Sassoon\n",
            "Episode 'Drill to Detail Ep.55 'Snowplow, Data Pipelines and Event-Level Digital Analytics' with Special Guest Yali Sassoon' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.54 'DataRobot & Machine Learning Automation' with Special Guest Greg Michaelson\n",
            "Episode 'Drill to Detail Ep.54 'DataRobot & Machine Learning Automation' with Special Guest Greg Michaelson' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.53 'ThoughtSpot, Search and AI-Powered BI' With Special Guest Doug Bordonaro\n",
            "Episode 'Drill to Detail Ep.53 'ThoughtSpot, Search and AI-Powered BI' With Special Guest Doug Bordonaro' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.52 'Lyft, Ride-Share Analytics and ETL Developer Productivity ' With Special Guest Mark Grover\n",
            "Episode 'Drill to Detail Ep.52 'Lyft, Ride-Share Analytics and ETL Developer Productivity ' With Special Guest Mark Grover' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.51 'Druid, Imply and OLAP Analysis on Event-Level Datasets' With Special Guest Fangjin Yang\n",
            "Episode 'Drill to Detail Ep.51 'Druid, Imply and OLAP Analysis on Event-Level Datasets' With Special Guest Fangjin Yang' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.50 'Agile BI, Karl Marx and Our Man from Moscow' With Special Guests Stewart Bryson and Alex Gorbachev\n",
            "Episode 'Drill to Detail Ep.50 'Agile BI, Karl Marx and Our Man from Moscow' With Special Guests Stewart Bryson and Alex Gorbachev' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.49 'Trifacta, Google Cloud Dataprep and Data Wranging for Data Engineers' With Special Guest Will Davis\n",
            "Episode 'Drill to Detail Ep.49 'Trifacta, Google Cloud Dataprep and Data Wranging for Data Engineers' With Special Guest Will Davis' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.48 'Mondrian OLAP, Apache Calcite and Database Dis-Aggregation' With Special Guest Julian Hyde\n",
            "Episode 'Drill to Detail Ep.48 'Mondrian OLAP, Apache Calcite and Database Dis-Aggregation' With Special Guest Julian Hyde' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.47 'Business Analytics 2018 Predictive and Best-Practice Christmas & New Year Special' With Special Guest Christian Berg\n",
            "Episode 'Drill to Detail Ep.47 'Business Analytics 2018 Predictive and Best-Practice Christmas & New Year Special' With Special Guest Christian Berg' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.46 'Market Trends and Findings from the BI Survey 17' With Special Guest Dr. Carsten Bange\n",
            "Episode 'Drill to Detail Ep.46 'Market Trends and Findings from the BI Survey 17' With Special Guest Dr. Carsten Bange' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.45 'Tellius, YellowFin and the State of AI in Analytics Today' With Special Guest Jen Underwood\n",
            "Episode 'Drill to Detail Ep.45 'Tellius, YellowFin and the State of AI in Analytics Today' With Special Guest Jen Underwood' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.44 'Pandas, Apache Arrow and In-Memory Analytics' With Special Guest Wes McKinney\n",
            "Episode 'Drill to Detail Ep.44 'Pandas, Apache Arrow and In-Memory Analytics' With Special Guest Wes McKinney' successfully written to BigQuery\n",
            "Processing episode: Drill to Detail Ep.43 'Oracle Analytics, Data Visualization Desktop 4.0 and The Art of Product Management' with Special Guest Mike Durran\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'Audio file is too short. Minimum audio length is 0.1 seconds.', 'type': 'invalid_request_error', 'param': 'file', 'code': 'audio_too_short'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a0df99c3fb31>\u001b[0m in \u001b[0;36m<cell line: 315>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a0df99c3fb31>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(feed_url, num_episodes)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mprocessed_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mprocessed_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mprocessed_episodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a0df99c3fb31>\u001b[0m in \u001b[0;36mprocess_episode\u001b[0;34m(client, episode)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mfull_transcript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mfull_transcript\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtranscript\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a0df99c3fb31>\u001b[0m in \u001b[0;36mtranscribe_audio\u001b[0;34m(client, filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranscribe_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         transcript = client.audio.transcriptions.create(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whisper-1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/audio/transcriptions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, file, model, language, prompt, response_format, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# multipart/form-data; boundary=---abc--\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mextra_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"multipart/form-data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;34m\"/audio/transcriptions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscription_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranscriptionCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 942\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Audio file is too short. Minimum audio length is 0.1 seconds.', 'type': 'invalid_request_error', 'param': 'file', 'code': 'audio_too_short'}}"
          ]
        }
      ]
    }
  ]
}